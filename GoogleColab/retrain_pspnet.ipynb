{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RetrainPSPNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EH6LvfJJq20y",
        "colab_type": "code",
        "outputId": "bb74fa95-7e06-4eed-d510-0cea3f7f3f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Download dataset and models from google drive\n",
        "################################################################################\n",
        "\n",
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "import os.path\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "dataset_training = drive.CreateFile({'id': '175QZrGu2KyjKITBcePwcyFzcL1zyJWi9'})\n",
        "dataset_training.GetContentFile(\"dataset2_training_0.zip\")\n",
        "\n",
        "dataset_validation = drive.CreateFile({'id': '1Hu4bm92fmnNNPlvweKUgdaPRkL_uN74Z'})\n",
        "dataset_validation.GetContentFile(\"dataset2_validation_0.zip\")\n",
        "\n",
        "model = drive.CreateFile({'id': '1_DaND4hvVBxS_is8GCZS-JSJLiDTYHRY'})\n",
        "model.GetContentFile(\"pspnet.h5\")\n",
        "\n",
        "model = drive.CreateFile({'id': '1YpCm6bho9fNCVgY9Bh8WBaXX5OQlFqI1'})\n",
        "model.GetContentFile(\"trained_pspnet.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 21.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.3)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v-fBEuOstQ8f",
        "colab_type": "code",
        "outputId": "e738619b-e117-4713-a7da-6768e6266a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Import libraries\n",
        "################################################################################\n",
        "\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import CSVLogger, Callback\n",
        "from keras import layers\n",
        "from keras.backend import tf as ktf, eval, set_value\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ISz4oksXtS1f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Generator and its functions\n",
        "################################################################################\n",
        "\n",
        "# returns x, y\n",
        "def batch_images(index, batch_size, image_paths, imgs, trained_model):\n",
        "    with imgs.open(image_paths[index]) as img:\n",
        "        img = Image.open(img).transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    img = img.resize((473, 473))\n",
        "\n",
        "    y = predict_segmentation(img, trained_model)\n",
        "    x = np.array(img.convert(\"L\").convert(\"RGB\")) - np.array([[[128, 128, 128]]])\n",
        "\n",
        "    return np.expand_dims(x, axis=0), y\n",
        "\n",
        "\n",
        "# Returns one-hot encoded segmentation object (w x h x 150)\n",
        "def predict_segmentation(img, trained_model):\n",
        "    data_mean = np.array([[[123.68, 116.779, 103.939]]])\n",
        "    pixel_img = np.array(img)\n",
        "    pixel_img = pixel_img - data_mean\n",
        "    bgr_img = pixel_img[:, :, ::-1]\n",
        "    segmented_img = trained_model.predict(np.expand_dims(bgr_img, axis=0))\n",
        "    return segmented_img\n",
        "\n",
        "\n",
        "# Yields batches of x and y values\n",
        "def generator_fn(batch_size, images_path, trained_model, validation=False):\n",
        "    with zipfile.ZipFile(images_path) as imgs:\n",
        "        image_paths = imgs.infolist()\n",
        "        n_images = len(image_paths)\n",
        "        i = 0\n",
        "        while True:\n",
        "            if i + batch_size > n_images:\n",
        "                i = 0\n",
        "            if validation and i + batch_size > 256:\n",
        "                i = 0\n",
        "            x, y = batch_images(i, batch_size, image_paths, imgs, trained_model)\n",
        "            i += batch_size\n",
        "            yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JtVEUodN9Rch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Loading, training and saving model\n",
        "################################################################################\n",
        "\n",
        "# custom pspnet layer\n",
        "class Interp(layers.Layer):\n",
        "\n",
        "    def __init__(self, new_size, **kwargs):\n",
        "        self.new_size = new_size\n",
        "        super(Interp, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Interp, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        new_height, new_width = self.new_size\n",
        "        resized = ktf.image.resize_images(inputs, [new_height, new_width],\n",
        "                                          align_corners=True)\n",
        "        return resized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.new_size[0], self.new_size[1], input_shape[3]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Interp, self).get_config()\n",
        "        config['new_size'] = self.new_size\n",
        "        return config\n",
        "\n",
        "\n",
        "# Returns pspnet\n",
        "def load_trained_model(path):\n",
        "    trained_model = load_model(path, custom_objects={'Interp': Interp})\n",
        "    trained_model._make_predict_function()\n",
        "    return trained_model\n",
        "\n",
        "  \n",
        "# Class for saving and uploading model & csv logger\n",
        "class Upload2Drive(Callback):\n",
        "    def __init__(self, model_name, n_epochs):\n",
        "        self.model_name = model_name\n",
        "        self.n_epochs = n_epochs\n",
        "        self.model_checkpoint = drive.CreateFile({\"title\": self.model_name + \".h5\",\n",
        "                                                \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                             \"id\": \"1b4yDZuEjuCDuEKybgyHBWX85ovIbjGAX\"}]})\n",
        "        self.model_log = drive.CreateFile({\"title\": self.model_name + \".csv\",\n",
        "                                          \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                       \"id\": \"1b4yDZuEjuCDuEKybgyHBWX85ovIbjGAX\"}]})\n",
        "  \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        try:\n",
        "            if gauth.access_token_expired:\n",
        "                gauth.Refresh()\n",
        "        except:\n",
        "            print(\"refresh failed\")\n",
        "        if True or epoch / self.n_epochs > 0.4:\n",
        "            try:\n",
        "                self.model.save(self.model_name + \".h5\", overwrite=True)\n",
        "                self.model_checkpoint.SetContentFile(self.model_name + \".h5\")\n",
        "                self.model_log.SetContentFile(self.model_name + \".csv\")\n",
        "            except:\n",
        "                print(\"save failed\")\n",
        "\n",
        "            try:\n",
        "                self.model_checkpoint.Upload()\n",
        "                self.model_log.Upload()\n",
        "            except:\n",
        "                print(\"upload checkpoint failed\")\n",
        "\n",
        "# Returns list of callbacks\n",
        "def callbacks(model_name, n_epochs):\n",
        "    cb = list()\n",
        "    cb.append(CSVLogger(model_name + \".csv\"))\n",
        "    cb.append(Upload2Drive(model_name, n_epochs))\n",
        "    return cb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYVphuxt9wrP",
        "colab_type": "code",
        "outputId": "58cf5a73-e611-48f2-f33c-929309bb2251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1427
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Main\n",
        "################################################################################\n",
        "\n",
        "model_name = \"pspnet_8\"\n",
        "batch_size = 1\n",
        "batches_per_epoch = 4672\n",
        "n_epochs = 10\n",
        "batches_per_validation = 256\n",
        "\n",
        "trained_model = load_trained_model(\"trained_pspnet.h5\")\n",
        "model = load_trained_model(\"pspnet.h5\")\n",
        "\n",
        "training_data_fn = generator_fn(batch_size, \"dataset2_training_0.zip\", trained_model)\n",
        "validation_data_fn = generator_fn(batch_size, \"dataset2_validation_0.zip\", trained_model, validation=True)\n",
        "\n",
        "start_time = time.time()\n",
        "model.fit_generator(training_data_fn,\n",
        "                    epochs=n_epochs,\n",
        "                    steps_per_epoch=batches_per_epoch,\n",
        "                    callbacks=callbacks(model_name, n_epochs),\n",
        "                    validation_data=validation_data_fn,\n",
        "                    validation_steps=batches_per_validation,\n",
        "                    verbose=2,\n",
        "                    max_queue_size=30)\n",
        "\n",
        "print(\"Training took: \" + str(time.time() - start_time))\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 5238s - loss: 1.6002 - acc: 0.6622 - val_loss: 1.6473 - val_acc: 0.6115\n",
            "Epoch 2/10\n",
            " - 5207s - loss: 1.6151 - acc: 0.6574 - val_loss: 1.6308 - val_acc: 0.6058\n",
            "Epoch 3/10\n",
            " - 5206s - loss: 1.5974 - acc: 0.6623 - val_loss: 1.6854 - val_acc: 0.5921\n",
            "Epoch 4/10\n",
            " - 5205s - loss: 1.6277 - acc: 0.6545 - val_loss: 1.7124 - val_acc: 0.5912\n",
            "Epoch 5/10\n",
            " - 5208s - loss: 1.6249 - acc: 0.6560 - val_loss: 1.6183 - val_acc: 0.6418\n",
            "Epoch 6/10\n",
            " - 5207s - loss: 1.6079 - acc: 0.6589 - val_loss: 1.6517 - val_acc: 0.5945\n",
            "Epoch 7/10\n",
            " - 5212s - loss: 1.6253 - acc: 0.6552 - val_loss: 1.5478 - val_acc: 0.6496\n",
            "Epoch 8/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f87dee504d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches_per_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     max_queue_size=30)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training took: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}