{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleColab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LTGX0pl4YSuP",
        "colab_type": "code",
        "outputId": "1039a034-a91d-4cd0-b5a3-bd0f136de21e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Download dataset and model from google drive to local storage\n",
        "################################################################################\n",
        "\n",
        "!ls\n",
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "import os.path\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# dataset_training = drive.CreateFile({'id': '1oqu3B_0gwRtVe___H7AH8wwxGzt25e3N'})\n",
        "# dataset_training.GetContentFile(\"coco_training_1.zip\")\n",
        "\n",
        "# dataset_validation = drive.CreateFile({'id': '19Vsr2lU_fQMD3HSJLfpd3E5GBXWRO9Jm'})\n",
        "# dataset_validation.GetContentFile(\"coco_validation_1.zip\")\n",
        "  \n",
        "dataset = drive.CreateFile({'id': '1Yo4xiohHHoRquswIda4yZP6QxHK3aWo_'})\n",
        "dataset.GetContentFile(\"places2_training_2.zip\") \n",
        "  \n",
        "# dataset = drive.CreateFile({'id': '17sUNxu0Eq7-sGih46QOVO1E62L6sw4Dp'})\n",
        "# dataset.GetContentFile(\"places2_training_1.zip\")\n",
        "\n",
        "dataset = drive.CreateFile({'id': '1zJVZMpxyHw7pScvnG-pVr5PDD9XbQqRd'})\n",
        "dataset.GetContentFile(\"places2_validation_1.zip\")\n",
        "\n",
        "# dataset = drive.CreateFile({'id': '1QO4NdHG99U6TVswmy4j3lz7ITxRp8D1B'})\n",
        "# dataset.GetContentFile(\"faces_in_wild.zip\")\n",
        "\n",
        "# dataset = drive.CreateFile({'id': '10zuh0FbTmgMF6lgUf_EHnP9CAdvaofxI'})\n",
        "# dataset.GetContentFile(\"celeba_training_2.zip\")\n",
        "\n",
        "# dataset = drive.CreateFile({'id': '1WQ6h5jQ9B4RCN7dDg2ySR5FRkK1EMfEl'})\n",
        "# dataset.GetContentFile(\"celeba_validation_1.zip\")\n",
        "\n",
        "# model = drive.CreateFile({'id': '16H2UavQKdC_T8OTAWM5Z8EwL9FyesCMH'})\n",
        "# model.GetContentFile(\"colorization_model.hdf5\")\n",
        "\n",
        "model = drive.CreateFile({'id': '1YpCm6bho9fNCVgY9Bh8WBaXX5OQlFqI1'})\n",
        "model.GetContentFile(\"pspnet.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SnN1WJPxzVwR",
        "colab_type": "code",
        "outputId": "eae77b7c-81c4-439f-eb51-5fc901fe59a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Import necessary libraries\n",
        "################################################################################\n",
        "\n",
        "# !pip install q keras==2.0.0\n",
        "# !pip install --upgrade Pillow\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2lab\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Conv2D, concatenate, Input, UpSampling2D\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras import layers\n",
        "from keras.backend import tf as ktf\n",
        "import keras\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "\n",
        "# config = ktf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "# k.tensorflow_backend.set_session(ktf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C5ZF5CXJCefx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Generator and its functions\n",
        "################################################################################\n",
        "\n",
        "# Returns numpy arrays l, a and b (w x h x 1)\n",
        "def lab_img(img):\n",
        "    if img.format != (\"JPEG\" or \"JPG\"):\n",
        "        img = img.convert(\"RGB\")\n",
        "    img = rgb2lab(img)\n",
        "\n",
        "    l = img[:, :, 0]\n",
        "    a = img[:, :, 1]\n",
        "    b = img[:, :, 2]\n",
        "\n",
        "    l = (np.array(l) / 100)\n",
        "    a = ((np.array(a) + 127) / 255) * 2 - 1\n",
        "    b = ((np.array(b) + 128) / 255) * 2 - 1\n",
        "\n",
        "    l = np.expand_dims(l, axis=2)\n",
        "    a = np.expand_dims(a, axis=2)\n",
        "    b = np.expand_dims(b, axis=2)\n",
        "\n",
        "    return l, a, b\n",
        "\n",
        "\n",
        "# Returns batch of x and y values packed together\n",
        "def batch_images(index, batch_size, images_size, image_paths, imgs, trained_model, flip):\n",
        "    x_batch = np.zeros((batch_size, images_size[1], images_size[0], 1))\n",
        "    s_batch = np.zeros((batch_size, int(images_size[1] / 8), int(images_size[0] / 8), 150))\n",
        "    y_batch = np.zeros((batch_size, images_size[1], images_size[0], 2))\n",
        "    for i in range(index, index + batch_size):\n",
        "        with imgs.open(image_paths[i]) as img:\n",
        "            img = Image.open(img)\n",
        "\n",
        "            if img.size != images_size:\n",
        "                img = img.resize(images_size)\n",
        "\n",
        "            s = predict_segmentation(img.convert(\"L\").convert(\"RGB\"), trained_model) \n",
        "#             s = predict_classification(img.convert(\"L\").convert(\"RGB\"), trained_model)\n",
        "            l, a, b = lab_img(img)\n",
        "\n",
        "            y = np.concatenate((a, b), axis=2)\n",
        "\n",
        "            x_batch[i - index] = l\n",
        "            s_batch[i - index] = s\n",
        "            y_batch[i - index] = y\n",
        "            \n",
        "            if flip:\n",
        "                x = np.fliplr(l)\n",
        "                s = np.fliplr(s)\n",
        "                y = np.fliplr(y)\n",
        "\n",
        "                x_batch[i - index] = x\n",
        "                s_batch[i - index] = s\n",
        "                y_batch[i - index] = y\n",
        "\n",
        "    return x_batch, s_batch, y_batch\n",
        "\n",
        "\n",
        "# Returns one-hot encoded segmentation object (w x h x 150)\n",
        "def predict_segmentation(img, trained_model):\n",
        "    # TODO: do this in my model\n",
        "    data_mean = np.array([[[123.68, 116.779, 103.939]]])\n",
        "    input_size = (473, 473)\n",
        "    output_size = (img.size[0] / 8, img.size[1] / 8)\n",
        "\n",
        "    if img.size != input_size:\n",
        "        img = img.resize(input_size)\n",
        "\n",
        "    pixel_img = np.array(img)\n",
        "    pixel_img = pixel_img - data_mean\n",
        "    bgr_img = pixel_img[:, :, ::-1]\n",
        "    segmented_img = trained_model.predict(np.expand_dims(bgr_img, axis=0))[0]\n",
        "#     segmented_img = segmented_img * 2 - 1  (in case of tanh)\n",
        "    if output_size != input_size:\n",
        "          segmented_img = resize(segmented_img,\n",
        "                                 (output_size[1], output_size[0], 150),\n",
        "                                  mode=\"constant\",\n",
        "                                  preserve_range=True)\n",
        "    return segmented_img\n",
        "\n",
        "  \n",
        "def predict_classification(img, trained_model):\n",
        "    output_size = (32, 32)\n",
        "    classification_img = trained_model.predict(np.expand_dims(np.array(img), axis=0))[0]\n",
        "    classification_img = resize(classification_img,\n",
        "                               (output_size[1], output_size[0], 1280),\n",
        "                               mode=\"constant\",\n",
        "                               preserve_range=True)\n",
        "    return classification_img\n",
        "\n",
        "\n",
        "# Yields batches of x and y values\n",
        "def generator_fn(batch_size, images_path, images_size, trained_model, flip=False, validation=False):\n",
        "    with zipfile.ZipFile(images_path) as imgs:\n",
        "        image_paths = imgs.infolist()\n",
        "        n_images = len(image_paths)\n",
        "        if validation:\n",
        "            n_images = batch_size\n",
        "#         print(n_images)\n",
        "        i = 0\n",
        "        while True:\n",
        "            if i + batch_size >= n_images:\n",
        "                i = 0\n",
        "#             print(\"batch start index: \" + str(i) + \"   \" + images_path)\n",
        "            x, s, y = batch_images(i, batch_size, images_size, image_paths, imgs, trained_model, flip)\n",
        "            i += batch_size\n",
        "            yield [x, s], y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fkbmf3XdCW5V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Loading, training and saving model\n",
        "################################################################################\n",
        "\n",
        "class Interp(layers.Layer):\n",
        "\n",
        "    def __init__(self, new_size, **kwargs):\n",
        "        self.new_size = new_size\n",
        "        super(Interp, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Interp, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        new_height, new_width = self.new_size\n",
        "        resized = ktf.image.resize_images(inputs, [new_height, new_width],\n",
        "                                          align_corners=True)\n",
        "        return resized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.new_size[0], self.new_size[1], input_shape[3]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Interp, self).get_config()\n",
        "        config['new_size'] = self.new_size\n",
        "        return config\n",
        "\n",
        "\n",
        "# Returns trained model instance\n",
        "def load_trained_model(path):\n",
        "    trained_model = load_model(path, custom_objects={'Interp': Interp})\n",
        "    trained_model._make_predict_function()\n",
        "    return trained_model\n",
        "\n",
        "\n",
        "# Returns main model\n",
        "def model_definition():\n",
        "    grayscale_input = Input(shape=(None, None, 1))\n",
        "    grayscale1 = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", strides=2)(grayscale_input)\n",
        "    grayscale2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(grayscale1)\n",
        "    grayscale3 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(grayscale2)\n",
        "    grayscale4 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(grayscale3)\n",
        "    r1 = layers.add([grayscale4, grayscale2])\n",
        "    grayscale5 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", strides=2)(r1)\n",
        "    grayscale6 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(grayscale5)\n",
        "    grayscale7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(grayscale6)\n",
        "    grayscale8 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(grayscale7)\n",
        "    r2 = layers.add([grayscale8, grayscale6])\n",
        "    grayscale9 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", strides=2)(r2)\n",
        "    grayscale10 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(grayscale9)\n",
        "    grayscale11 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(grayscale10)\n",
        "    grayscale12 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(grayscale11)\n",
        "    r3 = layers.add([grayscale12, grayscale10])\n",
        "\n",
        "    segmentation_input = Input(shape=(None, None, 150))\n",
        "\n",
        "    merged = concatenate([r3, segmentation_input], axis=3)\n",
        "    colorized1 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(merged)\n",
        "    colorized2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(colorized1)\n",
        "    colorized3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(colorized2)\n",
        "    r4 = layers.add([colorized3, colorized1])\n",
        "    colorized4 = UpSampling2D()(r4)\n",
        "    colorized5 = Conv2D(128, (9, 9), padding=\"same\", activation=\"relu\")(colorized4)\n",
        "    colorized6 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(colorized5)\n",
        "    colorized7 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(colorized6)\n",
        "    r5 = layers.add([colorized7, colorized5])\n",
        "    colorized8 = UpSampling2D()(r5)\n",
        "    colorized9 = Conv2D(64, (9, 9), padding=\"same\", activation=\"relu\")(colorized8)\n",
        "    colorized10 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(colorized9)\n",
        "    colorized11 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(colorized10)\n",
        "    r6 = layers.add([colorized11, colorized9])\n",
        "    colorized12 = UpSampling2D()(r6)\n",
        "    colorized13 = Conv2D(32, (9, 9), padding=\"same\", activation=\"relu\")(colorized12)\n",
        "    colorized14 = Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\")(colorized13)\n",
        "    colorized15 = Conv2D(2, (3, 3), padding=\"same\", activation=\"tanh\")(colorized14)\n",
        "\n",
        "    model = Model(inputs=[grayscale_input, segmentation_input], outputs=colorized15)\n",
        "    model.compile(loss=\"mse\", optimizer=Adam(lr=0.0004, decay=0.05))\n",
        "    return model\n",
        "\n",
        "  \n",
        "class Upload2Drive(Callback):\n",
        "  def __init__(self, generation, n_epochs):\n",
        "      self.generation = generation\n",
        "      self.n_epochs = n_epochs\n",
        "      self.model_checkpoint = drive.CreateFile({\"title\": str(self.generation) + \"th_gen.hdf5\",\n",
        "                                              \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                           \"id\": \"116i9g5Ee6_TyDd0EVLsL7l_EE24LV4Zo\"}]})\n",
        "      self.model_log = drive.CreateFile({\"title\": str(self.generation) + \"th_gen.csv\",\n",
        "                                        \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                     \"id\": \"1Gb601hJZ4fC_tcJ_rWC4-HLLYk6A8owi\"}]})\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      if epoch % 2 == 0 or epoch + 1 == self.n_epochs:\n",
        "          self.model.save(str(self.generation) + \"th_gen.hdf5\", overwrite=True)\n",
        "          self.model_checkpoint.SetContentFile(str(self.generation) + \"th_gen.hdf5\")\n",
        "          self.model_log.SetContentFile(str(self.generation) + \"th_gen.csv\")\n",
        "\n",
        "          if gauth.access_token_expired:\n",
        "              gauth.Refresh()\n",
        "          try:\n",
        "              self.model_checkpoint.Upload()\n",
        "              self.model_log.Upload()\n",
        "          except:\n",
        "              print(\"upload checkpoint failed\")\n",
        "\n",
        "# Returns list of used keras callbacks\n",
        "def callbacks(generation, n_epochs):\n",
        "    cb = list()\n",
        "    cb.append(CSVLogger(str(generation) + \"th_gen.csv\"))\n",
        "    cb.append(Upload2Drive(generation, n_epochs))\n",
        "    return cb\n",
        "\n",
        "\n",
        "# Returns the model after training it\n",
        "def train_model(model, training_data_fn, validation_data_fn, n_epochs, steps_per_epoch, validation_steps, generation):\n",
        "    model.fit_generator(training_data_fn,\n",
        "                        epochs=n_epochs,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        callbacks=callbacks(generation, n_epochs),\n",
        "                        validation_data=validation_data_fn,\n",
        "                        validation_steps=validation_steps,\n",
        "                        verbose=2,\n",
        "                        max_queue_size=1)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nmkMHGMOCjLd",
        "colab_type": "code",
        "outputId": "a91a9f81-8d32-4c68-9ecb-7bb1312097bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1890
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Control panel\n",
        "################################################################################\n",
        "\n",
        "generation = \"19\" # don't forget about this\n",
        "\n",
        "batch_size = 64 # twice as much gets trained if flip=True\n",
        "batches_per_epoch = 10\n",
        "n_epochs = 50\n",
        "batches_per_validation = 1\n",
        "images_size = (256, 256)\n",
        "\n",
        "training_path = \"places2_training_2.zip\"\n",
        "validation_path = \"places2_validation_1.zip\"\n",
        "trained_model = load_trained_model(\"pspnet.h5\")\n",
        "# trained_model = MobileNetV2(include_top=False)\n",
        "# trained_model._make_predict_function()\n",
        "model = model_definition()\n",
        "# model = load_model(\"colorization_model.hdf5\")\n",
        "\n",
        "training_data_fn = generator_fn(batch_size, training_path, images_size, trained_model, validation=False)\n",
        "validation_data_fn = generator_fn(batch_size, validation_path, images_size, trained_model, validation=True)\n",
        "\n",
        "start_time = time.time()\n",
        "train_model(model, \n",
        "            training_data_fn, \n",
        "            validation_data_fn, \n",
        "            n_epochs, \n",
        "            batches_per_epoch, \n",
        "            batches_per_validation, \n",
        "            generation)\n",
        "print(\"Training took: \" + str(time.time() - start_time))\n",
        "\n",
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " - 632s - loss: 0.0173 - val_loss: 0.0126\n",
            "Epoch 2/50\n",
            " - 556s - loss: 0.0118 - val_loss: 0.0120\n",
            "Epoch 3/50\n",
            " - 571s - loss: 0.0137 - val_loss: 0.0117\n",
            "Epoch 4/50\n",
            " - 558s - loss: 0.0120 - val_loss: 0.0116\n",
            "Epoch 5/50\n",
            " - 567s - loss: 0.0122 - val_loss: 0.0117\n",
            "Epoch 6/50\n",
            " - 562s - loss: 0.0125 - val_loss: 0.0115\n",
            "Epoch 7/50\n",
            " - 566s - loss: 0.0105 - val_loss: 0.0119\n",
            "Epoch 8/50\n",
            " - 562s - loss: 0.0116 - val_loss: 0.0114\n",
            "Epoch 9/50\n",
            " - 565s - loss: 0.0115 - val_loss: 0.0113\n",
            "Epoch 10/50\n",
            " - 561s - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 11/50\n",
            " - 566s - loss: 0.0116 - val_loss: 0.0111\n",
            "Epoch 12/50\n",
            " - 559s - loss: 0.0118 - val_loss: 0.0110\n",
            "Epoch 13/50\n",
            " - 565s - loss: 0.0110 - val_loss: 0.0110\n",
            "Epoch 14/50\n",
            " - 564s - loss: 0.0125 - val_loss: 0.0109\n",
            "Epoch 15/50\n",
            " - 570s - loss: 0.0113 - val_loss: 0.0109\n",
            "Epoch 16/50\n",
            " - 566s - loss: 0.0113 - val_loss: 0.0108\n",
            "Epoch 17/50\n",
            " - 569s - loss: 0.0111 - val_loss: 0.0109\n",
            "Epoch 18/50\n",
            " - 560s - loss: 0.0114 - val_loss: 0.0108\n",
            "Epoch 19/50\n",
            " - 572s - loss: 0.0105 - val_loss: 0.0110\n",
            "Epoch 20/50\n",
            " - 563s - loss: 0.0113 - val_loss: 0.0109\n",
            "Epoch 21/50\n",
            " - 569s - loss: 0.0118 - val_loss: 0.0108\n",
            "Epoch 22/50\n",
            " - 564s - loss: 0.0114 - val_loss: 0.0108\n",
            "Epoch 23/50\n",
            " - 570s - loss: 0.0108 - val_loss: 0.0108\n",
            "Epoch 24/50\n",
            " - 566s - loss: 0.0112 - val_loss: 0.0108\n",
            "Epoch 25/50\n",
            " - 571s - loss: 0.0108 - val_loss: 0.0109\n",
            "Epoch 26/50\n",
            " - 568s - loss: 0.0110 - val_loss: 0.0109\n",
            "Epoch 27/50\n",
            " - 573s - loss: 0.0110 - val_loss: 0.0109\n",
            "Epoch 28/50\n",
            " - 568s - loss: 0.0119 - val_loss: 0.0108\n",
            "Epoch 29/50\n",
            " - 573s - loss: 0.0114 - val_loss: 0.0107\n",
            "Epoch 30/50\n",
            " - 571s - loss: 0.0103 - val_loss: 0.0107\n",
            "Epoch 31/50\n",
            " - 574s - loss: 0.0120 - val_loss: 0.0107\n",
            "Epoch 32/50\n",
            " - 567s - loss: 0.0106 - val_loss: 0.0107\n",
            "Epoch 33/50\n",
            " - 572s - loss: 0.0110 - val_loss: 0.0106\n",
            "Epoch 34/50\n",
            " - 567s - loss: 0.0110 - val_loss: 0.0107\n",
            "Epoch 35/50\n",
            " - 572s - loss: 0.0109 - val_loss: 0.0107\n",
            "Epoch 36/50\n",
            " - 565s - loss: 0.0113 - val_loss: 0.0106\n",
            "Epoch 37/50\n",
            " - 573s - loss: 0.0110 - val_loss: 0.0106\n",
            "Epoch 38/50\n",
            " - 566s - loss: 0.0109 - val_loss: 0.0108\n",
            "Epoch 39/50\n",
            " - 571s - loss: 0.0113 - val_loss: 0.0106\n",
            "Epoch 40/50\n",
            " - 562s - loss: 0.0108 - val_loss: 0.0106\n",
            "Epoch 41/50\n",
            " - 570s - loss: 0.0112 - val_loss: 0.0105\n",
            "Epoch 42/50\n",
            " - 566s - loss: 0.0109 - val_loss: 0.0106\n",
            "Epoch 43/50\n",
            " - 572s - loss: 0.0105 - val_loss: 0.0108\n",
            "Epoch 44/50\n",
            " - 575s - loss: 0.0114 - val_loss: 0.0105\n",
            "Epoch 45/50\n",
            " - 578s - loss: 0.0114 - val_loss: 0.0105\n",
            "Epoch 46/50\n",
            " - 576s - loss: 0.0106 - val_loss: 0.0105\n",
            "Epoch 47/50\n",
            " - 579s - loss: 0.0115 - val_loss: 0.0105\n",
            "Epoch 48/50\n",
            " - 571s - loss: 0.0113 - val_loss: 0.0105\n",
            "Epoch 49/50\n",
            " - 581s - loss: 0.0108 - val_loss: 0.0106\n",
            "Epoch 50/50\n",
            " - 575s - loss: 0.0105 - val_loss: 0.0105\n",
            "Training took: 28668.295192718506\n",
            "19th_gen.csv   adc.json\t\t       places2_validation_1.zip  sample_data\n",
            "19th_gen.hdf5  places2_training_2.zip  pspnet.h5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}