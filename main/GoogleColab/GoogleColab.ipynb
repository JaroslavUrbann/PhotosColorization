{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleColab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LTGX0pl4YSuP",
        "colab_type": "code",
        "outputId": "a7655d85-8728-4055-a3cd-2dba4e8b2a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Download dataset and model from google drive to local storage\n",
        "################################################################################\n",
        "\n",
        "!ls\n",
        "!pip install PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "import os.path\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# # dataset_training = drive.CreateFile({'id': '1oqu3B_0gwRtVe___H7AH8wwxGzt25e3N'})\n",
        "# # dataset_training.GetContentFile(\"coco_training_1.zip\")\n",
        "\n",
        "# # dataset_validation = drive.CreateFile({'id': '19Vsr2lU_fQMD3HSJLfpd3E5GBXWRO9Jm'})\n",
        "# # dataset_validation.GetContentFile(\"coco_validation_1.zip\")\n",
        "  \n",
        "# dataset = drive.CreateFile({'id': '1Yo4xiohHHoRquswIda4yZP6QxHK3aWo_'})\n",
        "# dataset.GetContentFile(\"places2_training_2.zip\") \n",
        "  \n",
        "# # dataset = drive.CreateFile({'id': '17sUNxu0Eq7-sGih46QOVO1E62L6sw4Dp'})\n",
        "# # dataset.GetContentFile(\"places2_training_1.zip\")\n",
        "\n",
        "# dataset = drive.CreateFile({'id': '1zJVZMpxyHw7pScvnG-pVr5PDD9XbQqRd'})\n",
        "# dataset.GetContentFile(\"places2_validation_1.zip\")\n",
        "\n",
        "# # dataset = drive.CreateFile({'id': '1QO4NdHG99U6TVswmy4j3lz7ITxRp8D1B'})\n",
        "# # dataset.GetContentFile(\"faces_in_wild.zip\")\n",
        "\n",
        "# # dataset = drive.CreateFile({'id': '10zuh0FbTmgMF6lgUf_EHnP9CAdvaofxI'})\n",
        "# # dataset.GetContentFile(\"celeba_training_2.zip\")\n",
        "\n",
        "# # dataset = drive.CreateFile({'id': '1WQ6h5jQ9B4RCN7dDg2ySR5FRkK1EMfEl'})\n",
        "# # dataset.GetContentFile(\"celeba_validation_1.zip\")\n",
        "\n",
        "# model = drive.CreateFile({'id': '12T24din3lXrseomcsXAetJdiIm_2MQxc'})\n",
        "# model.GetContentFile(\"colorization_model.hdf5\")\n",
        "\n",
        "# # model = drive.CreateFile({'id': '1YpCm6bho9fNCVgY9Bh8WBaXX5OQlFqI1'})\n",
        "# # model.GetContentFile(\"pspnet.h5\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12th_gen.csv   adc.json\t\t\tplaces2_training_2.zip\t  sample_data\n",
            "12th_gen.hdf5  colorization_model.hdf5\tplaces2_validation_1.zip\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SnN1WJPxzVwR",
        "colab_type": "code",
        "outputId": "c93fbae1-d79d-40e5-ce7f-9d4ec9b17faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Import necessary libraries\n",
        "################################################################################\n",
        "\n",
        "# !pip install q keras==2.0.0\n",
        "# !pip install --upgrade Pillow\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2lab\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Conv2D, concatenate, Input, UpSampling2D\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
        "from keras import layers\n",
        "from keras.backend import tf as ktf\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "\n",
        "# config = ktf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "# k.tensorflow_backend.set_session(ktf.Session(config=config))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "C5ZF5CXJCefx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Generator and its functions\n",
        "################################################################################\n",
        "\n",
        "# Returns numpy arrays l, a and b (w x h x 1)\n",
        "def lab_img(img):\n",
        "    if img.format != (\"JPEG\" or \"JPG\"):\n",
        "        img = img.convert(\"RGB\")\n",
        "    img = rgb2lab(img)\n",
        "\n",
        "    l = img[:, :, 0]\n",
        "    a = img[:, :, 1]\n",
        "    b = img[:, :, 2]\n",
        "\n",
        "    l = (np.array(l) / 100)\n",
        "    a = ((np.array(a) + 127) / 255) * 2 - 1\n",
        "    b = ((np.array(b) + 128) / 255) * 2 - 1\n",
        "\n",
        "    l = np.expand_dims(l, axis=2)\n",
        "    a = np.expand_dims(a, axis=2)\n",
        "    b = np.expand_dims(b, axis=2)\n",
        "\n",
        "    return l, a, b\n",
        "\n",
        "\n",
        "# Returns batch of x and y values packed together\n",
        "def batch_images(index, batch_size, images_size, image_paths, imgs, trained_model, flip):\n",
        "    x_batch = np.zeros((batch_size, images_size[1], images_size[0], 1))\n",
        "    s_batch = np.zeros((batch_size, int(images_size[1] / 8), int(images_size[0] / 8), 1280))\n",
        "    y_batch = np.zeros((batch_size, images_size[1], images_size[0], 2))\n",
        "    for i in range(index, index + batch_size):\n",
        "        with imgs.open(image_paths[i]) as img:\n",
        "            img = Image.open(img)\n",
        "\n",
        "            if img.size != images_size:\n",
        "                img = img.resize(images_size)\n",
        "\n",
        "#             s = predict_segmentation(img.convert(\"L\").convert(\"RGB\"), trained_model) \n",
        "            s = predict_classification(img.convert(\"L\").convert(\"RGB\"), trained_model)\n",
        "            l, a, b = lab_img(img)\n",
        "\n",
        "            y = np.concatenate((a, b), axis=2)\n",
        "\n",
        "            x_batch[i - index] = l\n",
        "            s_batch[i - index] = s\n",
        "            y_batch[i - index] = y\n",
        "            \n",
        "            if flip:\n",
        "                x = np.fliplr(l)\n",
        "                s = np.fliplr(s)\n",
        "                y = np.fliplr(y)\n",
        "\n",
        "                x_batch[i - index] = x\n",
        "                s_batch[i - index] = s\n",
        "                y_batch[i - index] = y\n",
        "\n",
        "    return x_batch, s_batch, y_batch\n",
        "\n",
        "\n",
        "# Returns one-hot encoded segmentation object (w x h x 150)\n",
        "def predict_segmentation(img, trained_model):\n",
        "    # TODO: do this in my model\n",
        "    data_mean = np.array([[[123.68, 116.779, 103.939]]])\n",
        "    input_size = (473, 473)\n",
        "    output_size = (32, 32)\n",
        "\n",
        "    if img.size != input_size:\n",
        "        img = img.resize(input_size)\n",
        "\n",
        "    pixel_img = np.array(img)\n",
        "    pixel_img = pixel_img - data_mean\n",
        "    bgr_img = pixel_img[:, :, ::-1]\n",
        "    segmented_img = trained_model.predict(np.expand_dims(bgr_img, axis=0))[0]\n",
        "#     segmented_img = segmented_img * 2 - 1  (in case of tanh)\n",
        "    if output_size != input_size:\n",
        "          segmented_img = resize(segmented_img,\n",
        "                                 (output_size[1], output_size[0], 150),\n",
        "                                  mode=\"constant\",\n",
        "                                  preserve_range=True)\n",
        "    return segmented_img\n",
        "\n",
        "  \n",
        "def predict_classification(img, trained_model):\n",
        "    output_size = (32, 32)\n",
        "    classification_img = trained_model.predict(np.expand_dims(np.array(img), axis=0))[0]\n",
        "    classification_img = resize(classification_img,\n",
        "                               (output_size[1], output_size[0], 1280),\n",
        "                               mode=\"constant\",\n",
        "                               preserve_range=True)\n",
        "    return classification_img\n",
        "\n",
        "\n",
        "# Yields batches of x and y values\n",
        "def generator_fn(batch_size, images_path, images_size, trained_model, flip=False, validation=False):\n",
        "    with zipfile.ZipFile(images_path) as imgs:\n",
        "        image_paths = imgs.infolist()\n",
        "        n_images = len(image_paths)\n",
        "        if validation:\n",
        "            n_images = batch_size\n",
        "#         print(n_images)\n",
        "        i = 0\n",
        "        while True:\n",
        "            if i + batch_size >= n_images:\n",
        "                i = 0\n",
        "#             print(\"batch start index: \" + str(i) + \"   \" + images_path)\n",
        "            x, s, y = batch_images(i, batch_size, images_size, image_paths, imgs, trained_model, flip)\n",
        "            i += batch_size\n",
        "            yield [x, s], y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fkbmf3XdCW5V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Loading, training and saving model\n",
        "################################################################################\n",
        "\n",
        "class Interp(layers.Layer):\n",
        "\n",
        "    def __init__(self, new_size, **kwargs):\n",
        "        self.new_size = new_size\n",
        "        super(Interp, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(Interp, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        new_height, new_width = self.new_size\n",
        "        resized = ktf.image.resize_images(inputs, [new_height, new_width],\n",
        "                                          align_corners=True)\n",
        "        return resized\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.new_size[0], self.new_size[1], input_shape[3]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Interp, self).get_config()\n",
        "        config['new_size'] = self.new_size\n",
        "        return config\n",
        "\n",
        "\n",
        "# Returns trained model instance\n",
        "def load_trained_model(path):\n",
        "    trained_model = load_model(path, custom_objects={'Interp': Interp})\n",
        "    trained_model._make_predict_function()\n",
        "    return trained_model\n",
        "\n",
        "\n",
        "# Returns main model\n",
        "def model_definition():\n",
        "    grayscale_input = Input(shape=(None, None, 1))\n",
        "    grayscale = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True, strides=2)(grayscale_input)\n",
        "    grayscale = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(grayscale)\n",
        "    grayscale = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True, strides=2)(grayscale)\n",
        "    grayscale = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(grayscale)\n",
        "    grayscale = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True, strides=2)(grayscale)\n",
        "    grayscale = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(grayscale)\n",
        "    grayscale = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(grayscale)\n",
        "    grayscale = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(grayscale)\n",
        "\n",
        "    segmentation_input = Input(shape=(None, None, 1280))\n",
        "\n",
        "    merged = concatenate([grayscale, segmentation_input], axis=3)\n",
        "    colorized = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(merged)\n",
        "    colorized = UpSampling2D()(colorized)\n",
        "    colorized = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(colorized)\n",
        "    colorized = UpSampling2D()(colorized)\n",
        "    colorized = Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(colorized)\n",
        "    colorized = Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", use_bias=True)(colorized)\n",
        "    colorized = Conv2D(2, (3, 3), padding=\"same\", activation=\"tanh\", use_bias=True)(colorized)\n",
        "    colorized = UpSampling2D()(colorized)\n",
        "\n",
        "    model = Model(inputs=[grayscale_input, segmentation_input], outputs=colorized)\n",
        "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "    return model\n",
        "\n",
        "  \n",
        "class Upload2Drive(Callback):\n",
        "  def __init__(self, generation, n_epochs):\n",
        "      self.generation = generation\n",
        "      self.n_epochs = n_epochs\n",
        "      self.model_checkpoint = drive.CreateFile({\"title\": str(self.generation) + \"th_gen.hdf5\",\n",
        "                                              \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                           \"id\": \"116i9g5Ee6_TyDd0EVLsL7l_EE24LV4Zo\"}]})\n",
        "      self.model_log = drive.CreateFile({\"title\": str(self.generation) + \"th_gen.csv\",\n",
        "                                        \"parents\": [{\"kind\": \"drive#childList\",\n",
        "                                                     \"id\": \"1Gb601hJZ4fC_tcJ_rWC4-HLLYk6A8owi\"}]})\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "      if epoch % 2 == 0 or epoch + 1 == self.n_epochs:\n",
        "          self.model.save(str(self.generation) + \"th_gen.hdf5\", overwrite=True)\n",
        "          self.model_checkpoint.SetContentFile(str(self.generation) + \"th_gen.hdf5\")\n",
        "          self.model_log.SetContentFile(str(self.generation) + \"th_gen.csv\")\n",
        "\n",
        "          if gauth.access_token_expired:\n",
        "              gauth.Refresh()\n",
        "          try:\n",
        "              self.model_checkpoint.Upload()\n",
        "              self.model_log.Upload()\n",
        "          except:\n",
        "              print(\"upload checkpoint failed\")\n",
        "\n",
        "# Returns list of used keras callbacks\n",
        "def callbacks(generation, n_epochs):\n",
        "    cb = list()\n",
        "    cb.append(CSVLogger(str(generation) + \"th_gen.csv\"))\n",
        "    cb.append(Upload2Drive(generation, n_epochs))\n",
        "    return cb\n",
        "\n",
        "\n",
        "# Returns the model after training it\n",
        "def train_model(model, training_data_fn, validation_data_fn, n_epochs, steps_per_epoch, validation_steps, generation):\n",
        "    model.fit_generator(training_data_fn,\n",
        "                        epochs=n_epochs,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        callbacks=callbacks(generation, n_epochs),\n",
        "                        validation_data=validation_data_fn,\n",
        "                        validation_steps=validation_steps,\n",
        "                        verbose=2,\n",
        "                        max_queue_size=1)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nmkMHGMOCjLd",
        "colab_type": "code",
        "outputId": "24b789fc-4ada-40de-d83e-50a7b2ee5a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1122
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# Control panel\n",
        "################################################################################\n",
        "\n",
        "current_directory = \"\"\n",
        "generation = 12 # don't forget about this\n",
        "\n",
        "batch_size = 64 # twice as much gets trained if flip=True\n",
        "batches_per_epoch = 40\n",
        "n_epochs = 30\n",
        "batches_per_validation = 1\n",
        "images_size = (256, 256)\n",
        "\n",
        "training_path = \"places2_training_2.zip\"\n",
        "validation_path = \"places2_validation_1.zip\"\n",
        "# trained_model = load_trained_model(\"pspnet.h5\")\n",
        "trained_model = MobileNetV2(include_top=False)\n",
        "trained_model._make_predict_function()\n",
        "# model = model_definition()\n",
        "model = load_model(\"colorization_model.hdf5\")\n",
        "\n",
        "training_data_fn = generator_fn(batch_size, training_path, images_size, trained_model)\n",
        "validation_data_fn = generator_fn(batch_size, validation_path, images_size, trained_model, validation=True)\n",
        "\n",
        "start_time = time.time()\n",
        "train_model(model, \n",
        "            training_data_fn, \n",
        "            validation_data_fn, \n",
        "            n_epochs, \n",
        "            batches_per_epoch, \n",
        "            batches_per_validation, \n",
        "            generation)\n",
        "print(\"Training took: \" + str(time.time() - start_time))\n",
        "\n",
        "!ls\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet_v2.py:306: UserWarning: MobileNet shape is undefined. Weights for input shape(224, 224) will be loaded.\n",
            "  warnings.warn('MobileNet shape is undefined.'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            " - 482s - loss: 0.0116 - val_loss: 0.0114\n",
            "Epoch 2/30\n",
            " - 452s - loss: 0.0117 - val_loss: 0.0117\n",
            "Epoch 3/30\n",
            " - 460s - loss: 0.0115 - val_loss: 0.0114\n",
            "Epoch 4/30\n",
            " - 444s - loss: 0.0118 - val_loss: 0.0114\n",
            "Epoch 5/30\n",
            " - 460s - loss: 0.0113 - val_loss: 0.0114\n",
            "Epoch 6/30\n",
            " - 454s - loss: 0.0116 - val_loss: 0.0113\n",
            "Epoch 7/30\n",
            " - 461s - loss: 0.0116 - val_loss: 0.0113\n",
            "Epoch 8/30\n",
            " - 455s - loss: 0.0115 - val_loss: 0.0114\n",
            "Epoch 9/30\n",
            " - 460s - loss: 0.0114 - val_loss: 0.0112\n",
            "Epoch 10/30\n",
            " - 456s - loss: 0.0114 - val_loss: 0.0113\n",
            "Epoch 11/30\n",
            " - 459s - loss: 0.0116 - val_loss: 0.0112\n",
            "Epoch 12/30\n",
            " - 455s - loss: 0.0117 - val_loss: 0.0113\n",
            "Epoch 13/30\n",
            " - 461s - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 14/30\n",
            " - 453s - loss: 0.0113 - val_loss: 0.0113\n",
            "Epoch 15/30\n",
            " - 461s - loss: 0.0109 - val_loss: 0.0112\n",
            "Epoch 16/30\n",
            " - 456s - loss: 0.0115 - val_loss: 0.0113\n",
            "Epoch 17/30\n",
            " - 459s - loss: 0.0106 - val_loss: 0.0114\n",
            "Epoch 18/30\n",
            " - 455s - loss: 0.0114 - val_loss: 0.0113\n",
            "Epoch 19/30\n",
            " - 460s - loss: 0.0110 - val_loss: 0.0113\n",
            "Epoch 20/30\n",
            " - 455s - loss: 0.0111 - val_loss: 0.0113\n",
            "Epoch 21/30\n",
            " - 461s - loss: 0.0120 - val_loss: 0.0114\n",
            "Epoch 22/30\n",
            " - 456s - loss: 0.0111 - val_loss: 0.0112\n",
            "Epoch 23/30\n",
            " - 460s - loss: 0.0115 - val_loss: 0.0113\n",
            "Epoch 24/30\n",
            " - 455s - loss: 0.0114 - val_loss: 0.0112\n",
            "Epoch 25/30\n",
            " - 462s - loss: 0.0114 - val_loss: 0.0111\n",
            "Epoch 26/30\n",
            " - 455s - loss: 0.0112 - val_loss: 0.0111\n",
            "Epoch 27/30\n",
            " - 460s - loss: 0.0115 - val_loss: 0.0113\n",
            "Epoch 28/30\n",
            " - 454s - loss: 0.0112 - val_loss: 0.0113\n",
            "Epoch 29/30\n",
            " - 460s - loss: 0.0114 - val_loss: 0.0111\n",
            "Epoch 30/30\n",
            " - 455s - loss: 0.0115 - val_loss: 0.0112\n",
            "Training took: 13847.272961378098\n",
            "12th_gen.csv   adc.json\t\t\tplaces2_training_2.zip\t  sample_data\n",
            "12th_gen.hdf5  colorization_model.hdf5\tplaces2_validation_1.zip\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}